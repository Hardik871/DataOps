{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088a9f6e-6cb6-491e-bf91-e3f09924fbad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1fec61-8058-4d45-a814-1457ffc51fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from faker import Faker\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc64f669-9fb1-4bd5-9673-d045967c3511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install apscheduler,faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b162dd6a-4881-4768-8f38-39d71050e94e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from faker import Faker\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Configuration\n",
    "volume_path = os.getenv(\"VOLUME_PATH\", \"/Volumes/workspace/default/spark_df\")\n",
    "NUM_ROWS = int(os.getenv(\"NUM_ROWS\", 10))\n",
    "INTERVAL_MINUTES = int(os.getenv(\"INTERVAL_MINUTES\", 5))\n",
    "TIMEZONE = os.getenv(\"TIMEZONE\", \"UTC\")  # e.g., \"Asia/Kolkata\"\n",
    "\n",
    "SMTP_HOST = os.getenv(\"SMTP_HOST\", \"smtp.example.com\")\n",
    "SMTP_PORT = int(os.getenv(\"SMTP_PORT\", 587))\n",
    "SMTP_USER = os.getenv(\"SMTP_USER\", \"user@example.com\")\n",
    "SMTP_PASSWORD = os.getenv(\"SMTP_PASSWORD\", \"password\")\n",
    "EMAIL_FROM = os.getenv(\"EMAIL_FROM\", \"pipeline@example.com\")\n",
    "EMAIL_TO = os.getenv(\"EMAIL_TO\", \"recipient@example.com\")\n",
    "\n",
    "# Initialize Faker and Spark session\n",
    "faker = Faker()\n",
    "\n",
    "def generate_fake_data(n: int):\n",
    "    \"\"\"\n",
    "    Generate n rows of fake data (Name, Address, Email).\n",
    "    \"\"\"\n",
    "    data = [(faker.name(), faker.address().replace(\"\\n\", \", \"), faker.email()) for _ in range(n)]\n",
    "    return spark.createDataFrame(data, [\"Name\", \"Address\", \"Email\"])\n",
    "\n",
    "\n",
    "def append_to_delta(df):\n",
    "    \"\"\"\n",
    "    Append a DataFrame to the Delta table at volume_path, creating it if necessary.\n",
    "    \"\"\"\n",
    "    if DeltaTable.isDeltaTable(spark, volume_path):\n",
    "        df.write.format(\"delta\").mode(\"append\").save(volume_path)\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(volume_path)\n",
    "\n",
    "\n",
    "def get_latest_version():\n",
    "    \"\"\"\n",
    "    Retrieve the latest version and timestamp of the Delta table.\n",
    "    \"\"\"\n",
    "    dt = DeltaTable.forPath(spark, volume_path)\n",
    "    history_df = dt.history()  # full history as DataFrame\n",
    "    latest_row = history_df \\\n",
    "        .orderBy(col(\"timestamp\").cast(\"timestamp\").desc()) \\\n",
    "        .limit(1) \\\n",
    "        .collect()[0]\n",
    "    return latest_row[\"version\"], latest_row[\"timestamp\"]\n",
    "\n",
    "\n",
    "def read_latest_data():\n",
    "    \"\"\"\n",
    "    Read the full Delta table at its latest version.\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"delta\").load(volume_path)\n",
    "\n",
    "\n",
    "def send_email_notification(version, timestamp, new_count):\n",
    "    \"\"\"\n",
    "    Send an HTML email summarizing the latest ingestion.\n",
    "    \"\"\"\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = f\"Delta Ingestion Update - Version {version}\"\n",
    "    msg['From'] = EMAIL_FROM\n",
    "    msg['To'] = EMAIL_TO\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "      <body>\n",
    "        <h2>Delta Table Ingestion Summary</h2>\n",
    "        <p><strong>Version:</strong> {version}</p>\n",
    "        <p><strong>Timestamp:</strong> {timestamp}</p>\n",
    "        <p><strong>Rows Appended:</strong> {new_count}</p>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    part = MIMEText(html, 'html')\n",
    "    msg.attach(part)\n",
    "\n",
    "    with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:\n",
    "        server.starttls()\n",
    "        server.login(SMTP_USER, SMTP_PASSWORD)\n",
    "        server.sendmail(EMAIL_FROM, EMAIL_TO.split(\",\"), msg.as_string())\n",
    "\n",
    "\n",
    "def pipeline_job():\n",
    "    \"\"\"\n",
    "    The main pipeline job: generate data, append, track, notify.\n",
    "    \"\"\"\n",
    "    new_df = generate_fake_data(NUM_ROWS)\n",
    "    append_to_delta(new_df)\n",
    "\n",
    "    version, ts = get_latest_version()\n",
    "    appended_count = new_df.count()\n",
    "\n",
    "    # send_email_notification(version, ts, appended_count)\n",
    "\n",
    "    print(f\"[{datetime.now()}] Appended {appended_count} rows to Delta table at version {version}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_job()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48eaab7d-c706-4084-9c53-ca2ae4991a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if __name__ == \"__main__\":\n",
    "        pipeline_job()\n",
    "    print(\"Running Times\",i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f5fa65-e278-4368-8237-43f6338a7a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time data ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
